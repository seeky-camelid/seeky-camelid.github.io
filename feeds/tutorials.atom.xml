<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My test blog - Tutorials</title><link href="/" rel="alternate"></link><link href="/feeds/tutorials.atom.xml" rel="self"></link><id>/</id><updated>2021-07-11T17:36:00+00:00</updated><entry><title>Bayesian Statistics With PyStan - Installation</title><link href="/bayesian-statistics-with-pystan-installation.html" rel="alternate"></link><published>2021-07-11T17:36:00+00:00</published><updated>2021-07-11T17:36:00+00:00</updated><author><name>sicong</name></author><id>tag:None,2021-07-11:/bayesian-statistics-with-pystan-installation.html</id><summary type="html">&lt;div class="section" id="a-short-introduction-to-bayesian-statistics"&gt;
&lt;h2&gt;A Short Introduction to Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;This is the first in a part tutorial, part study notes, part rumination series on doing Bayesian Statistics with PyStan.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_statistics"&gt;Bayesian statistics&lt;/a&gt; is a fascinating paradigm of statistics that
takes on a subjective interpretation of probability, in contrast to the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_probability"&gt;Frequentist statistics&lt;/a&gt;
which is …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="a-short-introduction-to-bayesian-statistics"&gt;
&lt;h2&gt;A Short Introduction to Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;This is the first in a part tutorial, part study notes, part rumination series on doing Bayesian Statistics with PyStan.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_statistics"&gt;Bayesian statistics&lt;/a&gt; is a fascinating paradigm of statistics that
takes on a subjective interpretation of probability, in contrast to the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_probability"&gt;Frequentist statistics&lt;/a&gt;
which is supposed to offer a more &amp;quot;objective&amp;quot; view.&lt;/p&gt;
&lt;p&gt;This claim that Bayes is &amp;quot;subjective&amp;quot; makes more sense in the context of an inference problem. Suppose we want to model
some stochastic process with a model M, which depends on parameters P. And we observe some raw data, D. We then want to
estimate / infer the parameters P from the raw data D.&lt;/p&gt;
&lt;p&gt;The Frequentist approach assumes that there is one true, fixed parameter value for P, and our job is to approach this
ideal from the noisy and uncertain data D. However, the Bayesians instead assumes that P itself is uncertain whereas the
data D is treated as fixed. The best we can do is to collect more fixed data, to reduce our uncertainty in P.&lt;/p&gt;
&lt;p&gt;Furthermore, the Bayes approach is grounded firmly in the single most important corner stone in probability theory - the
Bayes' theorem&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
P(A \vert B) = \frac{P(A) * P(B \vert A)} {P(B)}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;In the context of inference, this turns into:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
P(Parameter \vert Data) = \frac{P(Parameter) * P(Data \vert Parameter)} {P(Data)}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Parameter \vert Data)\)&lt;/span&gt;, probability distribution of the Parameter given observed Data, is called the &lt;strong&gt;posterior&lt;/strong&gt; distribution,
and is the goal of Bayes inference.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Data \vert Parameter)\)&lt;/span&gt; is called the &lt;strong&gt;likelihood&lt;/strong&gt;. This is the data generation process and is usually the main process that
we want to model.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Parameter)\)&lt;/span&gt; is the &lt;strong&gt;prior&lt;/strong&gt;. This is the probability of the Parameter before Data is observed, hence why &amp;quot;prior&amp;quot;,
as in a priori knowledge / belief about the Parameter. This is the controversial bit of Bayes and is partly the reason why Bayes is said to be
&amp;quot;subjective&amp;quot;. Because the need for a prior implies that all parameters are fundamentally biased. There doesn't exist an ideal
where objective data alone gives us all the answer we want; the answer itself is always going to be tainted by some subjective
belief, the prior. All we can do is to weaken its influence with more data.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Data)\)&lt;/span&gt; is the &lt;strong&gt;denominator&lt;/strong&gt;, sometimes called &lt;strong&gt;marginal likelihood&lt;/strong&gt;. It's merely a normalization factor that
normalizes the numerator so that it remains a valid probability distribution. However, this is the main reason why exact Bayes inference
is hard.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bayes-statistics-and-computational-methods"&gt;
&lt;h2&gt;Bayes Statistics and Computational Methods&lt;/h2&gt;
&lt;p&gt;As stated above, the troublemaker is the denominator, because for continuous variables, it may involve &lt;em&gt;multi-parameter integration&lt;/em&gt;,
which can be very difficult to calculate if not intractable. We could use another technique called &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior"&gt;conjugate priors&lt;/a&gt;
to circumvent the issue. Nonetheless, this puts a restriction on what type of distributions we can use and thus only solves
the issue for special cases.&lt;/p&gt;
&lt;p&gt;Enter &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"&gt;Markov Chain Monte Carlo (MCMC)&lt;/a&gt;, a sampling-based computational
method to estimate the posterior. This topic deserves a post on its own. In short, instead of calculating the exact posterior
distribution, we &amp;quot;sample&amp;quot; from it, whose denominator we can ignore as its shape is governed by the numerator (prior * likelihood).
The exact algorithm varies across different &lt;em&gt;implementations&lt;/em&gt; of MCMC, the popular ones including &lt;strong&gt;Random Walk Metropolis&lt;/strong&gt;,
&lt;strong&gt;Gibbs Sampling&lt;/strong&gt; and &lt;strong&gt;Hamiltonian Monte Carlo&lt;/strong&gt;. However the principle is the same: we use &lt;em&gt;local decision rules&lt;/em&gt;
to accept or reject a sample point before moving on to the next one, so that &lt;em&gt;globally&lt;/em&gt; the distribution of the samples we accept,
follows the posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="stan-pystan"&gt;
&lt;h2&gt;Stan, PyStan&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://mc-stan.org/"&gt;Stan&lt;/a&gt; is a state-of-the-art platform for statistical modeling and high-performance statistical computation.
It implements a very fast MCMC sampling method called NUTS (No-U-Turn-Sampler) and interfaces with many languages like R,
Python MATLAB and so on.&lt;/p&gt;
&lt;p&gt;For this series I will try using Stan with Python via the &lt;a class="reference external" href="https://github.com/stan-dev/pystan"&gt;PyStan3&lt;/a&gt; open-source python package.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-install-pystan"&gt;
&lt;h2&gt;How to Install PyStan&lt;/h2&gt;
&lt;p&gt;PyStan has its &lt;a class="reference external" href="https://pystan.readthedocs.io/en/latest/"&gt;official guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The system requirements for the installation include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Python ≥3.7&lt;/li&gt;
&lt;li&gt;Linux or macOS&lt;/li&gt;
&lt;li&gt;x86-64 CPU&lt;/li&gt;
&lt;li&gt;C++ compiler: gcc ≥9.0 or clang ≥10.0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install, simply use pip:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 -m pip install pystan
&lt;/pre&gt;
&lt;div class="section" id="common-problems"&gt;
&lt;h3&gt;Common Problems&lt;/h3&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;RuntimeError when using pystan in Jupyter Notebook&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;There is a known error with using pystan3 in Jupyter Notebook. See &lt;a class="reference external" href="https://pystan.readthedocs.io/en/latest/faq.html"&gt;the FAQ session&lt;/a&gt;
of the official guide:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
RuntimeError: asyncio.run() cannot be called from a running event loop when running stan.build(...)
&lt;/pre&gt;
&lt;p&gt;This is because pystan3 relies on asyncio and Jupyter Notebook blocks some of its functionalities.&lt;/p&gt;
&lt;p&gt;To solve this:&lt;/p&gt;
&lt;ol class="last arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Install nest-asyncio&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Run this before importing pystan in your notebook:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import nest_asyncio
nest_asyncio.apply()
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stay tuned for more to come!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Tutorials"></category><category term="Bayesian"></category><category term="Python"></category></entry></feed>