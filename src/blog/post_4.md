---
title: "Musing: Embodied AI may require lifelong, online learning"
---

Half reflecting, half predicting, mainly targeted at my ideal embodied agent vs the current mainstream foundation models.

I think training / retraining a model offline on curated datasets then deploying it statically will be a thing of the past, at least for embodied agents that are meant to interact with the real world.

## The real world dynamics are not stationary
This lemma basically rules out the "train once, deploy indefinitely" methodology (or the zero-shot "learning" with methods like prompt engineering) as viable for embodied A(G)I. For a finite model in an unpredictably non-stationary world, real learning (i.e. the adjustment of the model's dynamics) needs to occur.

For neural network models, this means an embodied AI will always need downstream retraining for it to be practical in non-trivial use cases, no matter how powerful and comprehensive the underlying foundation model is.

Therefore, just like how a healthy and functional human intelligence needs lifelong adaptation and learning (Neuroplasticity), I'd argue that a functional embodied AI would also need to acquire this much more flexible and superior method: lifelong online learning from "lived" experiences.


## The real world dynamics are not unitary
This is one of my other gripes with the current status of large foundation models. They assumes a single, unified, almost omnipresent locus of human knowledge. Or rather, it eliminates any individual perspective of the world. This is why it is so hard / impossible for a foundation model to take a real stance on any particular problem.  

However, an embodied agent inherently requires a position and a perspective in the world; it inherently needs an "I" to function. This cannot be handed down by the foundation models; it can only be learned throughout the "lifetime" of the agent.